<!--
	作者：Sariay
	时间：2018-08-26
	描述：There may be a bug, but don't worry, Qiling(器灵) says that it can work normally! aha!
-->
<!DOCTYPE html>
<html class="html-loading">
		

<head>
	<meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <title>
    
      nlp | the log of study from jy
    
  </title>
  <meta name="author" content="Jiang Yi">
  <meta name="keywords" content="" />
  <meta name="description" content="" />
	<!-- favicon -->
  <link rel="shortcut icon" href="/img/favicon.ico">

  <!-- css -->
  
<link rel="stylesheet" href="/css/Annie.css">

  
  <!-- jquery -->
	
<script src="/plugin/jquery/jquery.min.js"></script>


<script>
    const CONFIG_BGIMAGE = {
      mode: 'random_you',
      normalSrc: '/img/header-bg.jpg',
      randomYouMax: 110,
      randomYouSrc: '/img/random/',
	  randomOtherSrc: 'https://api.berryapi.net/?service=App.Bing.Images&day=-0',
	  preloaderEnable: true
    }
	
    const CONFIG_LEACLOUD_COUNT = {
      enable: true,
	  appId: 'WQuHQElpTp6jLxEWsLhDituW-gzGzoHsz',
	  appKey: 'VPXaorbr5NOqXHb8gmA3WGip',
	  serverURLs: 'https://wquhqelp.lc-cn-n1-shared.com' || ' '
    }
  </script>
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="the log of study from jy" type="application/atom+xml">
</head>
	<body>
		<!-- Preloader -->

	<div id="preloader">
		<div class="pre-container">
			
				<div class="spinner">
					<div class="double-bounce1"></div>
					<div class="double-bounce2"></div>
				</div>
						
		</div>
	</div>


<!-- header -->
<header class="fixbackground bg-pan-br">
	<div class="mask">
		<!-- motto -->
		<div class="h-body">	
			
				<div class="motto text-shadow-pop-left">
					<p class="content" id="motto-content">Fetching...</p>
					<p>-<p>
					<p class="author" id="motto-author">Just a minute...</p>
				</div>
			
		</div>
		
		<!-- others: such as time... -->			
		<div class="h-footer">
			<a href="javascript:;" id="read-more" class="scroll-down">
				<span class="icon-anchor1 animation-scroll-down"></span>
			</a>
		</div>
	</div>
</header>

<div id="navigation-hide">
	<!-- Progress bar -->
	<div id="progress-bar"></div>

	<!-- Progress percent -->
	<div id="progress-percentage"><span>0.0%</span></div>

	<div class="toc-switch"><span class="switch-button">Catalog</span></div>

	<!-- Page title -->
	<p>
		
			「nlp」
		
	</p>

	
	
		<span id="star-button" rel="unlike">
			<i class="icon-heart"></i>
		</span>
	

	<!-- Nav trigger for navigation-H-->
	<a class="nav-trigger"><span></span></a>
</div>

<!-- Navigation in div(id="navigation-H") -->
<nav class="nav-container" id="cd-nav">
	<div class="nav-header">
		<span class="logo"> 
			<img src="/img/logo.png">
		</span>
		<a href="javascript:;" class="nav-close"></a>
	</div>
	
	<div class="nav-body">
		<ul id="global-nav">
	
		<li class="menu-home">
			<a href="/" class="menu-item-home" target="_blank">Home</a>
		</li>
		
	
		<li class="menu-archive">
			<a href="/archives" class="menu-item-archive" target="_blank">Archive</a>
		</li>
		
	
		<li class="menu-categories">
			<a href="/categories" class="menu-item-categories" target="_blank">Category</a>
		</li>
		
	
		<li class="menu-tags">
			<a href="/tags" class="menu-item-tags" target="_blank">Tag</a>
		</li>
		
	
		<li class="menu-about">
			<a href="/about" class="menu-item-about" target="_blank">About</a>
		</li>
		
	
		<li class="menu-gallery">
			<a href="/gallery" class="menu-item-gallery" target="_blank">Gallery</a>
		</li>
		
	

	
		<li class="menu-search">
			<a href="javascript:;" class="popup-trigger">Search</a>
		</li>
	
</ul>
	</div>
	
	<div class="nav-footer">
		<ul id="global-social">
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-one"><span class="path1"></span><span class="path2"></span></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-zhihu"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-github"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-sina-weibo "></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-pinterest2"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-instagram"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-twitter"></span>
			</a>
		</li>
	
		<li>
			<a href="/atom.xml" target="_blank">
				<span class="icon-rss"></span>
			</a>
		</li>
			
</ul>

	</div>
</nav>
			
		<!--main-->
		<main>
			<!--
	时间：2018-11-17
	描述：
		插件名称：katelog.min.js
		插件作者：KELEN
		插件来源: https://github.com/KELEN/katelog
-->

	
		<div class="layout-toc">
			<div id="layout-toc">
				<div class="k-catelog-list" id="catelog-list" data-title="Catalog"></div>
			</div>
		</div>

		
<script src="/plugin/toc/katelog.min.js"></script>


		
	 

<div class="layout-post">
	<div id="layout-post">
		<div class="article-title">
			
	<a href="/2021/05/15/nlp/" itemprop="url">
		nlp
	</a>

		</div>

		<div class="article-meta">
			<span>
				<i class="icon-calendar1"></i>
				
				




	Updated on

	<a href="/2021/05/15/nlp/" itemprop="url">
		<time datetime="2021-05-15T12:04:11.713Z" itemprop="dateUpdated">
	  		2021-05-22
	  </time>
	</a> 



			</span>
			<span>
				
	<i class="icon-price-tags"></i>
	
		<a href="/tags/ai/" class=" ">
			ai
		</a>
	
		
			</span>
			
			

	
    <span class="leancloud_visitors" id="/2021/05/15/nlp/_visitors" data-url="/2021/05/15/nlp/" data-title="nlp">
       	<i class="icon-eye"></i>
       	Hits
        
            <i class="leancloud_visitors_count" id="leancloud_visitors_count">1</i>
                   
    </span>
    



	
    <span class="leancloud_likes" id="/2021/05/15/nlp/_likes" data-url="/2021/05/15/nlp/" data-title="nlp" rel="unlike">
        <i class="icon-heart"></i>
        Likes
        <i class="leancloud_likes_count" id="leancloud_likes_count">0</i>
    </span>
	

		</div>

		<div class="article-content" id="article-content">
			<h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><p>有关pytorch的学习网站：<a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">https://pytorch-cn.readthedocs.io/zh/latest/</a><br>另外一些有关pytorch的知识点如下</p>
<h2 id="PyTorch中的Tensor张量"><a href="#PyTorch中的Tensor张量" class="headerlink" title="PyTorch中的Tensor张量"></a>PyTorch中的Tensor张量</h2><h3 id="1-Tensor张量"><a href="#1-Tensor张量" class="headerlink" title="1.Tensor张量"></a>1.Tensor张量</h3><p>我们可以将这个理解为矩阵</p>
<h3 id="2-Tensor数据类型"><a href="#2-Tensor数据类型" class="headerlink" title="2.Tensor数据类型"></a>2.Tensor数据类型</h3><p>（1）torch.FloatTensor()参数可以直接是一个元组然后转化为Tensor；或者是两个参数，n，m直接代表矩阵的维度已经形状。<br>（2）torch.IntTensor，用于生成数据类型为整形的tensor。</p>
<h3 id="3-Tensor常用函数"><a href="#3-Tensor常用函数" class="headerlink" title="3.Tensor常用函数"></a>3.Tensor常用函数</h3><p>（1）torch.rand    # 用于生成数据类型为浮点型且维度指定的随机Tensor，和在Numpy中使用numpy.rand生成随机数的方法类似，随机生成的浮点数据在0~1区间均匀分布。</p>
<p>（2）torch.randn    # 用于生成数据类型为浮点型且维度指定的随机Tensor，和在Numpy中使用numpy.randn生成随机数的方法类似，随机生成的浮点数的取值满足均值为0，方差为1的正太分布。</p>
<p>（3）torch.range    # 用于生成数据类型为浮点型且自定义其实范围和结束范围的Tensor，所以传递给torch.range的参数有三个，分别是范围的起始值，范围的结束值和步长，其中，步长用于指定从起始值到结束值的每步的数据间隔。</p>
<p>（4）torch.zeros    # 用于生成数据类型为浮点型且维度指定的Tensor，不过这个浮点型的Tensor中的元素值全部为0。</p>
<p>（5）torch.abs    # 将参数传递到torch.abs后返回输入参数的绝对值作为输出，输出参数必须是一个Tensor数据类型的变量。</p>
<p>（6）torch.add    # 将参数传递到torch.add后返回输入参数的求和结果作为输出，输入参数既可以全部是Tensor数据类型的变量，也可以是一个Tensor数据类型的变量，另一个是标量。</p>
<p>（7）torch.clamp    # 对输入参数按照自定义的范围进行裁剪，最后将参数裁剪的结果作为输出。所以输入参数一共有三个，分别是需要进行裁剪的Tensor数据类型的变量、裁剪的上边界和裁剪的下边界，具体的裁剪过程是：使用变量中的每个元素分别和裁剪的上边界及裁剪的下边界的值进行比较，如果元素的值小于裁剪的下边界的值，该元素就被重写成裁剪的下边界的值；同理，如果元素的值大于裁剪的上边界的值，该元素就被重写成裁剪的上边界的值。</p>
<p>（8）torch.div    # 将参数传递到torch.div后返回输入参数的求商结果作为输出，同样，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。(两个参数，第一个是被除数，第二个是除数)</p>
<p>（9）torch.pow    # 将参数传递到torch.pow后返回输入参数的求幂结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。</p>
<p>（10）torch.mul    # 将参数传递到 torch.mul后返回输入参数求积的结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。</p>
<p>（11）torch.mm    # 将参数传递到 torch.mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch.mul运算方式不太样，torch.mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。</p>
<p>（12）torch.mv    # 将参数传递到torch.mv后返回输入参数的求积结果作为输出，torch.mv运用矩阵与向量之间的乘法规则进行计算，被传入的参数中的第1个参数代表矩阵，第2个参数代表向量，顺序不能颠倒。<br>（以上基本都是二元运算，前面的参数是被运算的数，后面的是对前面多运算的参数,或者这个torch的实例直接去运行)</p>
<p>（13）torch.view    # 改变一个 tensor 的大小或者形状。</p>
<h2 id="神经网络堆叠"><a href="#神经网络堆叠" class="headerlink" title="神经网络堆叠"></a>神经网络堆叠</h2><h3 id="简易神经网络的堆叠"><a href="#简易神经网络的堆叠" class="headerlink" title="简易神经网络的堆叠"></a>简易神经网络的堆叠</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line">import torch</span><br><span class="line">batch_n = 100  <span class="comment">#一个批次中输入数据的数量,值是100表示在一个批次中输入100个数据</span></span><br><span class="line">input_data = 1000  <span class="comment">#每个数据包含的数据特征</span></span><br><span class="line">hidden_layer = 100 <span class="comment">#经过隐藏层后保留的数据特征的个数</span></span><br><span class="line">output_data = 10  <span class="comment">#输出数据表示分类结果值的数量</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#初始化权重</span></span><br><span class="line">x = torch.randn(batch_n, input_data) <span class="comment">#输入层维度为（100,1000）</span></span><br><span class="line">y = torch.randn(batch_n, output_data) <span class="comment">#输出层维度为（100,10）</span></span><br><span class="line">w1 = torch.randn(input_data, hidden_layer) <span class="comment">#输入层到隐藏层的权重参数维度为（1000,100）</span></span><br><span class="line">w2 = torch.randn(hidden_layer, output_data) <span class="comment">#隐藏层到输出层的权重参数维度为（100,10）</span></span><br><span class="line">epoch_n = 20  <span class="comment">#训练的次数</span></span><br><span class="line">learning_rate = 1e-6 <span class="comment">#学习效率</span></span><br><span class="line"><span class="comment">#梯度下降优化神经网络参数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epoch_n):</span><br><span class="line">    h1 = x.mm(w1)  <span class="comment"># 100*1000</span></span><br><span class="line">    h1 = h1.clamp(min=0) <span class="comment">#使用clamp方法进行裁剪，将小于零的值全部重新赋值于0</span></span><br><span class="line">    y_pred = h1.mm(w2)  <span class="comment"># 100*10</span></span><br><span class="line">    <span class="comment"># print(y_pred)</span></span><br><span class="line">    loss = (y_pred - y).pow(2).sum()  <span class="comment">#均方误差计算损失</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125; , Loss:&#123;:.4f&#125;&quot;</span>.format(epoch, loss))</span><br><span class="line">    gray_y_pred = 2 * (y_pred - y)</span><br><span class="line">    gray_w2 = h1.t().mm(gray_y_pred)  <span class="comment">#.t()是将Tensor进行转置</span></span><br><span class="line">    grad_h = gray_y_pred.clone()</span><br><span class="line">    grad_h = grad_h.mm(w2.t())</span><br><span class="line">    grad_h.clamp_(min=0)</span><br><span class="line">    grad_w1 = x.t().mm(grad_h)  <span class="comment">#权重参数对应的梯度</span></span><br><span class="line">    w1 -= learning_rate * grad_w1 <span class="comment">#根据学习率对w1和w2的权重参数进行更新</span></span><br><span class="line">    w2 -= learning_rate * gray_w2</span><br></pre></td></tr></table></figure>

<h3 id="2-Pytorch自动梯度"><a href="#2-Pytorch自动梯度" class="headerlink" title="2.Pytorch自动梯度"></a>2.Pytorch自动梯度</h3><p>autograd package 是PyTorch中所有神经网络的核心，它提供了Tensors上所有运算的自动求导功能，通过torch.autograd包，可以使模型参数自动计算在优化过程中需要用到的梯度值，在很大程度上帮助降低了实现后向传播代码的复杂度。</p>
<p>torch.autograd 包的主要功能是完成神经网络后向传播中的链式求导。</p>
<p>autograd.Variable 是这个package的中心类。它打包了一个Tensor，并且支持几乎所有运算。一旦你完成了你的计算，可以调用.backward()，所有梯度就可以自动计算。</p>
<h3 id="3-使用自动梯度和自定义函数搭建简易神经网络模型"><a href="#3-使用自动梯度和自定义函数搭建简易神经网络模型" class="headerlink" title="3.使用自动梯度和自定义函数搭建简易神经网络模型"></a>3.使用自动梯度和自定义函数搭建简易神经网络模型</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"> </span><br><span class="line"><span class="comment">#构建神经网络模型</span></span><br><span class="line">class Model(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Model,self).__init__()</span><br><span class="line">        <span class="comment">#super().__init__()</span></span><br><span class="line">    def forward(self,input,w1,w2):</span><br><span class="line">        x=torch.mm(input,w1)</span><br><span class="line">        x=torch.clamp(x,min=0)</span><br><span class="line">        x=torch.mm(x,w2)</span><br><span class="line">        <span class="built_in">return</span> x</span><br><span class="line">    def backward(self):</span><br><span class="line">        pass</span><br><span class="line"> </span><br><span class="line">batch_n = 100 <span class="comment"># 批量输入的数据量</span></span><br><span class="line">hidden_layer = 100 <span class="comment"># 通过隐藏层后输出的特征数</span></span><br><span class="line">input_data = 1000 <span class="comment"># 输入数据的特征个数</span></span><br><span class="line">output_data = 10 <span class="comment"># 最后输出的分类结果数</span></span><br><span class="line"><span class="comment">#初始化权重</span></span><br><span class="line">x = Variable(torch.randn(batch_n, input_data), requires_grad=False)</span><br><span class="line">y = Variable(torch.randn(batch_n, output_data), requires_grad=False)</span><br><span class="line">w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=True)</span><br><span class="line">w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=True)</span><br><span class="line">epoch_n = 20  <span class="comment">#训练的次数</span></span><br><span class="line">learning_rate = 1e-6 <span class="comment">#学习效率</span></span><br><span class="line">model=Model() <span class="comment">#对模型类进行调用</span></span><br><span class="line"><span class="comment">#模型训练和参数优化</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epoch_n):</span><br><span class="line">    y_pred = model(x,w1,w2) <span class="comment">#完成对模型预测值的输出</span></span><br><span class="line">    loss = (y_pred - y).pow(2).sum()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125; , Loss:&#123;:.4f&#125;&quot;</span>.format(epoch, loss.data))</span><br><span class="line">    loss.backward()  <span class="comment">#自动计算所有梯度</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h1 id="词向量学习"><a href="#词向量学习" class="headerlink" title="词向量学习"></a>词向量学习</h1><h2 id="One-hot编码"><a href="#One-hot编码" class="headerlink" title="One hot编码"></a>One hot编码</h2><h3 id="什么是One-hot编码"><a href="#什么是One-hot编码" class="headerlink" title="什么是One hot编码"></a>什么是One hot编码</h3><p>one-hot编码，又称独热编码、一位有效编码。其方法是使用N位状态寄 存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在 任意时候，其中只有一位有效。举个例子，假设我们有四个样本(行)， 每个样本有三个特征(列)，如下图:</p>
<table>
<thead>
<tr>
<th></th>
<th>Feature_1</th>
<th>Feature_2</th>
<th>Feature_3</th>
</tr>
</thead>
<tbody><tr>
<td>Sample1</td>
<td>1</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>Sample2</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>Sample3</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>Sample4</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>上图中我们已经对每个特征进行了普通的数字编码:我们的feature_1有两种可能的取值，比如是男/ 女，这里男用1表示，女用2表示。</p>
<p>那么one-hot编码是怎么搞的呢?我们再拿feature_2来说明:这里 feature_2 有4种取值(状态)，我们就用4个状态位来表示这个特 征，one-hot编码就是保证每个样本中的单个特征只有1位处于状态 1，其他的都是0。</p>
<p>​    1-&gt;0001</p>
<p>​    2-&gt;0010</p>
<p>​    3-&gt;0100</p>
<p>​    4-&gt;1000</p>
<p>对于2种状态、3种状态、甚至更多状态都可以这样表示，所以我们可以 得到这些样本特征的新表示，入下图:</p>
<table>
<thead>
<tr>
<th></th>
<th>Feature_1</th>
<th>Feature_2</th>
<th>Feature_3</th>
</tr>
</thead>
<tbody><tr>
<td>Sample1</td>
<td>01</td>
<td>1000</td>
<td>100</td>
</tr>
<tr>
<td>Sample2</td>
<td>10</td>
<td>0100</td>
<td>010</td>
</tr>
<tr>
<td>Sample3</td>
<td>01</td>
<td>0010</td>
<td>010</td>
</tr>
<tr>
<td>Sample4</td>
<td>10</td>
<td>0001</td>
<td>001</td>
</tr>
</tbody></table>
<p>one-hot编码将每个状态位都看成一个特征。对于前两个样本我们可以得到它的特征向量分别为</p>
<p>​    Sample_1-&gt;[0,1,1,0,0,0,1,0,0]</p>
<p>​    Sample_2-&gt;[1,0,0,1,0,0,0,1,0]</p>
<h3 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><p>1.解决了分类器不好处理离散数据的问题<br>2.在一定程度上也起到了扩充特征的作用</p>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><p>1.在文本特征上缺点就非常突出了<br>2.它是一个提词袋不考虑词与词之间的顺序<br>3.他假 设词与词相互独立（在大多数情况下，词与词是相互影响的）<br>4.他得到的特征是稀疏的</p>
<h2 id="word2vec相关概念"><a href="#word2vec相关概念" class="headerlink" title="word2vec相关概念"></a>word2vec相关概念</h2><p>2013年Google团队发表了word2vec工具。word2vec工具主要包含两个模型：跳字模型（skip-gram）和连续词袋模型（continuous bag of words，简称CBOW），以及两种高效训练的方法：负采样（negative samoling）和层序softmax（hierarchical softmax），值得一提的是，word2vec词向量可以较好的表达不同词之间类似和类比关系。</p>
<h2 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h2><h3 id="跳字模型："><a href="#跳字模型：" class="headerlink" title="跳字模型："></a>跳字模型：</h3><p>在跳字模型中，我们用一个词来预测它在文本序列周围的词。例如，给定文本“the”，“main”，“hit”，“his”和“son”，跳字模型所关心的是，给定“hit”，生成它的临近词“the”，“main”，“hit”和“son”的概率</p>
<p><img src="/images/skip-gram-1.PNG" alt="skip-gram-1"></p>
<p><img src="/images/skip-gram-2.PNG" alt="skip-gram-2"></p>
<p>上述公式中，T表示窗口中心词的位置，m表示的窗口的大小。这样就可以计算出每个中心词推断背景词的概率，而我们在输入的时候给出了背景词的向量，此时只需要最大化背景词的输出概率即可。 基于这样的想法，我们会想到极大化似然估计的方式。但是一个函数的最大值往往不容易计算，因此，我们可以通过对函数进行变换，从而改变函数的增减性，以便优化。</p>
<p>注解：看到这里，就引出了word2vec的核心方法，其实就是认为每个词相互独立，用连乘来估计最大似然函数，求解目标函数就是最大化似然函数。上面公式涉及到一个中心词向量v，以及背景词向量u，因此呢很有趣的是，可以用一个input-hidden-output的三层神经网络来建模上面的skip-model。</p>
<p>Skip-gram可以表示为由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络（示意图如下）：<br><img src="/images/skip-gram-3.jpg" alt="skip-gram-3"></p>
<p><img src="/images/skimgram.png" alt="skipgram"></p>
<p>1.输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0。<br>2.网络中传播的前向过程：输出层向量的值可以通过隐含层向量（K维），以及连接隐藏层和输出层之间的KxN维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。<br>训练：<br><img src="/images/skip-gram-4.PNG" alt="skip-gram-4"></p>
<p>注解：上面的公式每一步都推荐推到一下，都很基础。上面只求了对中心词向量v的梯度，同理对背景词向量的梯度，也很容易计算出来。然后就采用传统的梯度下降（一般采用sgd）来训练词向量（其实我们最后关心的是中心词向量来作为词的表征。）</p>
<p>这里有一个key point说下，也许大家也想到了：词向量到底在哪里呢？回看下前面图1，有两层神经网络，第一层是input层到hidden层，这个中间的weight矩阵就是词向量！！！</p>
<p>假设input的词汇表N长度是10000，hidden层的长度K=300，左乘以一个10000长度one-hot编码，实际上就是在做一个查表！因此，这个weight矩阵的行就是10000个词的词向量。很有创意是不是？再来看神经网络的第二层，hidden到out层，中间hidden层有没有激活函数呢？从前面建模看到，u和v是直接相乘的，没有激活层，所以hidden是一个线性层。out层就是建模了v和u相乘，结果过一个softmax，那么loss函数是最大似然怎么办呢？其实就是接多个只有一个true label（背景词）的cross entropy loss，把这些loss求和。因为交叉熵就是最大似然估计，如果这点不清楚的可以去网上搜一下，很容易知道。</p>
<p>所以呢，我们即可用前面常规的最大似然建模来理解如何对u和v的进行优化求解；也完全可以把skip-model套到上面图1这样的一个简单神经网络中，然后就让工具自己来完成weight的训练，就得到了我们想要的中心词向量。</p>
<p><img src="/images/skip-gram-5.jpg" alt="skip-gram-5"></p>
<p>下图是对这个过程的简单可视化过程示意图。左边矩阵为词汇表中第四个单词的one-hot表示，右边矩阵表示包含3个神经元的隐藏层的权重矩阵，做矩阵乘法的结果就是从权重矩阵中选取了第四行的权重。因此，这个隐藏层的权重矩阵就是我们最终想要获得的词向量字典[2]</p>
<p><img src="/images/skip-gram-6.jpg" alt="skip-gram-6"></p>
<p>获取训练样本：按照上下文窗口的大小从训练文本中提取出词汇对，下面以句子The quick brown fox jumps over the lazy dog为例提取用于训练的词汇对，然后将词汇对的两个单词使用one-hot编码就得到了训练用的train_data和target_data。 下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。Training Samples（输入， 输出）示意图如下：</p>
<p><img src="/images/skip-gram-7.jpg" alt="skip-gram-7"></p>
<p>如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。</p>
<h3 id="连续词袋模型（CBOW）"><a href="#连续词袋模型（CBOW）" class="headerlink" title="连续词袋模型（CBOW）"></a>连续词袋模型（CBOW）</h3><p>CBOW就倒过来，用多个背景词来预测一个中心词，CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好[2]。但是方法和上面是很像的，因此这里我就放下图。推导的方法是一样的。</p>
<p><img src="/images/CBOW-1.PNG" alt="CBOW-1"></p>
<p><img src="/images/CBOW-2.PNG" alt="CBOW-2"><br>我们发现，不论是跳字模型（skip-gram） 还是连续词袋模型（CBOW），我们实际上都是取得input-hidden这个词向量（weight矩阵），而不是后面带着loss那一部分，这样我们也很容易可以对loss（训练方法）进行修改，这个也是下一篇要说的内容：</p>
<p>问：每次梯度的计算复杂度是多少？当词典很大时，会有什么问题？</p>
<p>word2vec作者很创造性地提出了2种近似训练方法（分层softmax（hierarchical softmax）和负采样（negative sampling）），得益于此，可以训练大规模语料库。</p>
<h2 id="wordvrc优化方式"><a href="#wordvrc优化方式" class="headerlink" title="wordvrc优化方式"></a>wordvrc优化方式</h2><h3 id="近似训练"><a href="#近似训练" class="headerlink" title="近似训练"></a>近似训练</h3><h4 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h4><p><img src="/images/negative-sampling-1.png" alt="negative-sampling-1"></p>
<p><img src="/images/negative-sampling-2.png" alt="negative-sampling-2"></p>
<p><img src="/images/negative-sampling-3.png" alt="negative-sampling-3"></p>
<h4 id="层序softmax"><a href="#层序softmax" class="headerlink" title="层序softmax"></a>层序softmax</h4><p>一、h-softmax</p>
<p>在面对label众多的分类问题时，fastText设计了一种hierarchical softmax函数。使其具有以下优势：</p>
<p>（1）适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”，特别是与深度模型对比，fastText能将训练时间由数天缩短到几秒钟。<br>（2）支持多语言表达：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。<br>可以认为，FastText= （word2vec中）CBOW + h-softmax；其结构为：输入 - 隐层 - h-softmax</p>
<p>基本原理</p>
<p>根据标签（label）和频率建立霍夫曼树；（label出现的频率越高，Huffman树的路径越短）<br>Huffman树中每一叶子结点代表一个label；
 </p>
<p>二、理论分析</p>
<p>层次之间的映射</p>
<p>将输入层中的词和词组构成特征向量，再将特征向量通过线性变换映射到隐藏层，隐藏层通过求解最大似然函数，然后根据每个类别的权重和模型参数构建Huffman树，将Huffman树作为输出。 </p>
<p>模型的训练</p>
<p>Huffman树中每一叶子结点代表一个label，在每一个非叶子节点处都需要作一次二分类，走左边的概率和走右边的概率，这里用逻辑回归的公式表示 </p>
<p>模型的训练</p>
<p>Huffman树中每一叶子结点代表一个label，在每一个非叶子节点处都需要作一次二分类，走左边的概率和走右边的概率，这里用逻辑回归的公式表示 </p>
<p>模型的训练</p>
<p>Huffman树中每一叶子结点代表一个label，在每一个非叶子节点处都需要作一次二分类，走左边的概率和走右边的概率，这里用逻辑回归的公式表示</p>
<p><img src="/images/h-softmax.jpg" alt="h-softmax"> </p>
<p><img src="/images/h-softmax-1.png" alt="h-softmax-1"></p>
<p>how fast</p>
<p><img src="/images/h-softmax-2.png" alt="h-softmax-2"></p>
<h2 id="word2vec实现方式"><a href="#word2vec实现方式" class="headerlink" title="word2vec实现方式"></a>word2vec实现方式</h2><h2 id="word2vec实战"><a href="#word2vec实战" class="headerlink" title="word2vec实战"></a>word2vec实战</h2><h2 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h2><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><p>共现概率：<br>什么是共现？<br>  单词 i i i出现在单词 j j j的环境中(论文给的环境是以 j j j为中心的左右10个单词区间)叫共现。</p>
<p>什么是共现矩阵？<br>  单词对共现次数的统计表。我们可以通过大量的语料文本来构建一个共现统计矩阵。<br>  例如，有语料如下：<br>  I like deep learning<br>  I like NLP<br>  I enjoy flying<br>  我以窗半径为1来指定上下文环境，则共现矩阵就应该是</p>
<table>
<thead>
<tr>
<th>count</th>
<th>I</th>
<th>like</th>
<th>enjoy</th>
<th>deep</th>
<th>learning</th>
<th>NLP</th>
<th>flying</th>
</tr>
</thead>
<tbody><tr>
<td>I</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>like</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>enjoy</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>deep</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>learning</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>NLP</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>flying</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p>因为我们的窗口半径为1，所以对于上面我们的每一句语句来说，进行分析然后统计来看，最后出来的表上诉</p>
<p><img src="/images/glove-1.png" alt="glove-1"></p>
<p><img src="/images/glove-2.png" alt="glove-2"></p>
<p><img src="/images/glove-3.png" alt="glove-3"></p>
<p><img src="/images/glove-4.png" alt="glove-4"></p>
<p><img src="/images/glove-5.png" alt="glove-5"></p>
<p><img src="/images/glove-6.png" alt="glove-6"></p>
<h2 id="fasttext"><a href="#fasttext" class="headerlink" title="fasttext"></a>fasttext</h2>	
		</div>
		
		<div id="current-post-cover" data-scr="/images/2.jpg"></div>

		<!-- relate post, comment...-->
		<div class="investment-container">
			<div class="investment-header">
				<div class="investment-title-1">
					<div class="on">Related post</div>
					<div>Comment</div>
					<div>Share</div>
				</div>
				<div class="investment-title-2">	            
					
	<span>
		<a id="totop-post-page">To Top</a>
		
			<a href="/2021/07/16/git-use/" title="git-use" rel="prev">
				&laquo;Pre post
			</a>
		
		
			<a href="/2021/05/15/tomcat/" title="tomcat" rel="next">
				Next post&raquo;
			</a>
			
	</span>


      		
				</div>	
			</div>
			
			<div class="investment-content">
				<div class="investment-content-list">
					

<div class="relate-post">
	
		<ul>
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/05/15/tomcat/" title="tomcat">
								tomcat			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								五月 15日, 2021				
							</p>
							<p class="relate-post-content">
								
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/05/15/tomcat/" title="tomcat">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/5.jpg" alt="tomcat"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/05/11/nginx-use/" title="nginx-use">
								nginx-use			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								五月 11日, 2021				
							</p>
							<p class="relate-post-content">
								nginx使用教程本nginx是在Ubuntu20.04系统下运行的nginx version: nginx/1.18.0 (Ubuntu)
安装：1.安装nginx的方法直接apt install nginx就可以我们下面来看详细配...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/05/11/nginx-use/" title="nginx-use">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/2.jpg" alt="nginx-use"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/05/08/hello-world/" title="hello hexo">
								hello hexo			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								五月 8日, 2021				
							</p>
							<p class="relate-post-content">
								本篇文章介绍了如何使用hexo博客以及配置相关的内容（包括统计插件，评论插件等）：
安装并发布hexo安装hexo：npm install -g hexo-cli检查是否安装成功：hexo -v初始化网址：hexo init接着输入 ...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/05/08/hello-world/" title="hello hexo">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/1.jpg" alt="hello hexo"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/05/11/ai-study/" title="ai-study">
								ai-study			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								五月 11日, 2021				
							</p>
							<p class="relate-post-content">
								关于ai学习的一些历程
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/05/11/ai-study/" title="ai-study">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/4.jpeg" alt="ai-study"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/05/11/ai-env/" title="ai-environment">
								ai-environment			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								五月 11日, 2021				
							</p>
							<p class="relate-post-content">
								这个文档主要与pytorch等机器学习等python或者其他库等配置方法，和在服务器中的使用方法
用nvidia-smi来查看显卡的信息
接着用yum来安装python
安装annocoda环境，先下载包，然后bash即可，会自动帮你...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/05/11/ai-env/" title="ai-environment">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/3.jpeg" alt="ai-environment"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/07/18/springboot-use/" title="springboot-use">
								springboot-use			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								七月 18日, 2021				
							</p>
							<p class="relate-post-content">
								后端学习日志：SpringBootMVC的结构解读：对于SpringBoot来说一个高内聚低耦合的框架必须要遵守一个能够承受得住较大量开发的逻辑难度，有些开发者是单人开发，所面临的主要开发问题是如何记住自己写过的每一个功能，并且某些功...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/07/18/springboot-use/" title="springboot-use">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/8.jpg" alt="springboot-use"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/07/16/git-use/" title="git-use">
								git-use			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								七月 16日, 2021				
							</p>
							<p class="relate-post-content">
								在Mac的终端上输入git检测是否安装git，如果没有，点击弹出的“安装”按钮。安装完成之后，在终端输入 git –version 查看版本信息12git --version
创建一个全局用户名、全局邮箱作为配置信息12git con...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/07/16/git-use/" title="git-use">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/7.jpg" alt="git-use"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2021/05/10/unix-order/" title="unix-order">
								unix-order			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								五月 10日, 2021				
							</p>
							<p class="relate-post-content">
								UnixUnix 常用命令技巧（可配 linux 大部分命令）pwd：print work directory 打印工作目录
ls：查看当前文件夹文件 
​    -l：查看文件     -al：查看左右文件（包含以小数点开头的隐藏文...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2021/05/10/unix-order/" title="unix-order">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/6.jpg" alt="unix-order"/>
							</a>
						</div>
					</li>												
			
		</ul>
	
</div>	
				</div>
				<div class="investment-content-list">
					<div class="layout-comment">

	

		

			<!-- gitalk comment -->
			<!-- show gitalk comment -->
<div id="gitalk-container"></div>

<!-- gitalk`s css & js -->
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<link rel="stylesheet" href="/css/comment.css">
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<script type="text/javascript">

	(function gitalkComment(){
		//Thanks O-R
		//https://github.com/gitalk/gitalk/issues/102#issuecomment-382970552
		//去除尾部匹配正则数组的字符串  
		//Remove redundant characters
		String.prototype.trimEnd = function(regStr) {
			let result = this;
			if(regStr == undefined || regStr == null || regStr == "") {
				return result;
			}
			let array = regStr.split(',');

			if(array.length > 0) {

				let c = array.shift(), 
					str = this,
					i = str.length,
					rg = new RegExp(c),
					matchArr = str.match(rg);

				if(matchArr != undefined && matchArr != null && matchArr.length > 0) {
					let matchStr = matchArr[0].replace(/\\/g, "\\\\").replace(/\*/g, "\\*")
						.replace(/\+/g, "\\+").replace(/\|/g, "\\|")
						.replace(/\{/g, "\\{").replace(/\}/g, "\\}")
						.replace(/\(/g, "\\(").replace(/\)/g, "\\)")
						.replace(/\^/g, "\\^").replace(/\$/g, "\\$")
						.replace(/\[/g, "\\[").replace(/\]/g, "\\]")
						.replace(/\?/g, "\\?").replace(/\,/g, "\\,")
						.replace(/\./g, "\\.").replace(/\&/g, "\\&");
					matchStr = matchStr + '$';
					result = str.replace(new RegExp(matchStr), "");
				}

				if(array.length > 0) {
					return result.trimEnd(array.join())
				} else {
					return result;
				}
			}
		};

		//Create gitalk
		let gitalk = new Gitalk({
			clientID: '6905883cfd344fe4e388',
			clientSecret: '5d7d7318c2440628d5b5160c21b32cfc9c768a61',
			//id: window.location.pathname,
			//id: decodeURI(window.location.pathname),
			//id: (window.location.pathname).split("/").pop().substring(0, 49),
			id: decodeURI( md5( location.href.trimEnd('#.*$,\\?.*$,index.html$') ) ),
			repo: 'Blog-Comment-JY',
			owner: 'jiangyi01',
			admin: 'jiangyi01',
			distractionFreeMode: 'true',
		})
		gitalk.render('gitalk-container');		
	})();
</script>

		
		
	

</div>
				</div>
				<div class="investment-content-list">
					<div class="layout-share">
	
	

		
			
			<!-- socialShare share -->
			<div class="social-share"></div>

<!--  css & js -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
			
		
		
	
</div>


				</div>
			</div>	
		</div>
	</div>
</div>

<!-- show math formula -->



	 
	
<script src="/plugin/clipboard/clipboard.js"></script>

	<script>
		// Copy code !
	    function preprocessing() {
	        $("#article-content .highlight").each(function() {
	            $(this).wrap('<div id="post-code"></div>');
	        })

	        $("#article-content #post-code").each(function() {
	            $(this).prepend('<nav class="copy-nav"><span><i class="code-language"></i></span></nav>');
	        })

	        $("#article-content .copy-nav").each(function() {
	            let languageClass = $(this).next().attr('class'),
	                language = ((languageClass.length > 9) && (languageClass != null)) ? languageClass.substr(10) : "none"; //why 9? Need to check language?

	            $(this).find('.code-language').text(language);
	            $(this).append('<span class="copy-btn icon-paste"></span>');
	        });
	    }

		function copy() {
		    $('#article-content #post-code').each(function(i) {
		        let codeCopyId = 'codeCopy-' + i;

		        let codeNode = $(this).find('.code'),
		            copyButton = $(this).find('.copy-btn');

		        codeNode.attr('id', codeCopyId);
		        copyButton.attr('data-clipboard-target-id', codeCopyId);
		    })
   
			let clipboard = new ClipboardJS('.copy-btn', {
					target: function(trigger) {
						return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
					}
		      	});

			//pure js
			function showTooltip(elem, msg) {		   
				elem.setAttribute('aria-label', msg);
				elem.setAttribute('class', 'copy-btn icon-clipboard1');
				setTimeout(function() {
					elem.setAttribute('class', 'copy-btn icon-paste');
				}, 2000);
			}

			clipboard.on('success', function(e) {
			    e.clearSelection();
			    console.info('Action:', e.action);		   
			    console.info('Trigger:', e.trigger);
			    showTooltip(e.trigger, 'Copied!');   
			});
			
			clipboard.on('error', function(e) {
			    console.error('Action:', e.action);
			    console.error('Trigger:', e.trigger);
			});
		}
		
		(function copyCode(){
			if ($('.layout-post').length) {
			    preprocessing();
			    copy();
			} 
		})();
	</script>






<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">


<script src="/plugin/fancybox/jquery.fancybox.js"></script>


<script type="text/javascript">
	(function gallerySet(){
		let titleID = $('.article-title a'),
			imageID = $('.article-content img'),
			videoID = $('.article-content video');
		
		let postTitle = titleID.text() ? titleID.text() : "No post title!";
		
		imageID.each(function() {
			let imgPath = $(this).attr('src'),
				imgTitle = $(this).attr('alt') ? $(this).attr('alt') : "No image description!";
		
			//给每个匹配的<img>元素打包, 即添加父元素<a>
			$(this).wrap('<a data-fancybox="gallery" data-caption="《 ' + postTitle + ' 》' + imgTitle + '"href="' + imgPath + '"> </a>');
		});
		
		videoID.each(function() {
			let videoPath = $(this).attr('src');
		
			//给每个匹配的<img>元素打包, 即添加父元素<a>
			$(this).wrap('<a data-fancybox href=" ' + videoPath + ' "> </a>');
		});
		
		//TODO：支持html5 video

		if($('#layout-post').length) {
			$('[data-fancybox="gallery"]').fancybox({
				loop: true,
				buttons: [
					"zoom",
					"share",
					"slideShow",
					"fullScreen",
					//"download",
					"thumbs",
					"close"
				],
				protect: true
			});
		}
	})();
</script>
		</main>

		<!--footer-->
		<footer>
	<div id="navigation-show">
		<ul id="global-nav">
	
		<li class="menu-home">
			<a href="/" class="menu-item-home" target="_blank">Home</a>
		</li>
		
	
		<li class="menu-archive">
			<a href="/archives" class="menu-item-archive" target="_blank">Archive</a>
		</li>
		
	
		<li class="menu-categories">
			<a href="/categories" class="menu-item-categories" target="_blank">Category</a>
		</li>
		
	
		<li class="menu-tags">
			<a href="/tags" class="menu-item-tags" target="_blank">Tag</a>
		</li>
		
	
		<li class="menu-about">
			<a href="/about" class="menu-item-about" target="_blank">About</a>
		</li>
		
	
		<li class="menu-gallery">
			<a href="/gallery" class="menu-item-gallery" target="_blank">Gallery</a>
		</li>
		
	

	
		<li class="menu-search">
			<a href="javascript:;" class="popup-trigger">Search</a>
		</li>
	
</ul>
	</div>

	<div class="copyright">
		<p>
			 
				&copy;2021, content by JiangYi. All Rights Reserved.
			
			
				<a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> Theme <a href="https://github.com/Sariay/hexo-theme-Annie" title="Annie" target="_blank" rel="noopener">Annie</a> by Sariay.
			
		</p>
		<p>
			

	<!-- busuanzi -->
	<!-- busuanzi -->

		
	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	

		<span id="busuanzi_container_page_pv">
	  		本文总阅读量<span id="busuanzi_value_page_pv"></span>次
		</span>

	




			<a href="javascript:zh_tran('s');" class="zh_click" id="zh_click_s">简体</a> 
			<a href="javascript:zh_tran('t');" class="zh_click" id="zh_click_t">繁體</a>				
		</p>
	</div>		
</footer>
		
	<!-- Local or hitokoto! -->

	
<script src="/plugin/motto/motto.js"></script>

	
	<script type="text/javascript">
		(function motto(){
			let mottoText = getMingYanContent().split('</br> - </br>'),
			
			mottoTextContent = mottoText[0]?mottoText[0]:'请刷新...',
			
			mottoTextFrom = mottoText[1]?mottoText[1]:'one/一个';
			
			mottoTextContent = mottoTextContent.trim().substring(0, 100);
		
			$("#motto-content").html( mottoTextContent);
			$("#motto-author").html( mottoTextFrom  );
		})();	
	</script>	



<!-- love effect -->


<!-- back to top -->

	<div id="totop">
	<span class="icon-circle-up"></span>
</div>



<!-- site analysis -->


	<!-- site-analysis -->
	
	
	
	
	
 

<!-- leancloud -->


	<!-- leancloud -->
	<!--
	时间：2018-11-27
	描述：
		文章访问量：visitors
		文章喜欢量：likes	
		文章排行榜：topNPost
		其他得说明：
			01-Cookie相关的函数 
				https://blog.csdn.net/somehow1002/article/details/78511541（Author：somehow1002）
			02-visitors相关的函数 
				https://blog.csdn.net/u013553529/article/details/63357382（Author：爱博客大伯）
				https://notes.doublemine.me/2015-10-21-为NexT主题添加文章阅读量统计功能.html（Author：夏末）
			03-topNPost相关的函数
				https://hoxis.github.io/hexo-next-read-rank.html（Author：hoxis）
			04-likes相关的函数，
				参考了01 & 02进行简单的设计与实现
-->


  
<script src="/plugin/leancloud/av-min.js"></script>
<script src="/js/leancloud-count.js"></script>


	

  

	<!--
	时间：2018-10-3
	描述：
		插件名称：hexo-generator-search-zip
		插件来源: https://github.com/SuperKieran/hexo-generator-search-zip
		代码参考：https://github.com/SuperKieran/TKL/blob/master/layout/_partial/search.ejs(Include: js & css)	
-->
<div class="popup search-popup local-search-popup scrollbar" >
	<div class="local-search-container">
		<span class="popup-btn-close">
      		ESC
   		</span>
		<div class="local-search-header">
			<div class="input-prompt">				
			</div>
			<input autocomplete="off" placeholder="Search..." type="text" id="local-search-input">
		</div>
		<div class="local-search-body">
			<div id="local-search-output"></div>
		</div>
		<div class="local-search-footer">
			<div class="topN-post">				
				

   
	<div id="topN">
		<div class="topN-title" data-title= "Popular post"></div> 
	</div>
	
    <script>
        var limitCount = 10;
        if( $('#topN').length ){
            setTimeout(function() {
                topNPost(limitCount);
			}, 3000);
        }
    </script>
   
								
			</div>
		</div>
	</div>
</div>


<script src="/plugin/search/ziploader.js"></script>
<script src="/js/search.js"></script>


<script type="text/javascript">
	var search_path = 'search.json',
		zip_Path = '/search.zip',
		version_Path = '/searchVersion.txt',
		input_Trigger = 'auto',
		top_N = '2';

	themeLocalSearch({
		search_path, 
		zip_Path, 
		version_Path, 
		input_Trigger, 
		top_N
	});
</script>



<script src="/plugin/chinese/chinese.js"></script>
<script src="/plugin/imagelazyloader/yall.min.js"></script>
<script src="/plugin/imageloaded/imagesloaded.pkgd.min.js"></script>
<script src="/plugin/nicescroll/jquery.nicescroll.js"></script>
<script src="/plugin/resizediv/resizediv.js"></script>
<script src="/js/main.js"></script>

	<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>	
</html>